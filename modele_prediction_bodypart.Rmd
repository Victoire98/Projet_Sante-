---
title: "R Notebook"
output: html_notebook
---

**Note** : 

#### Dans la base de données originale, il n'y a que 31 diagnostics reconnus, ce n'est pas une bonne idée de les laisser dans une seule colonne. Cela génère plus de 15 000 diagnostics 
```{r}
library("dplyr")
#date
library("lubridate")
#data manipulation,
library(tidyr)
library(ggplot2)
library(doParallel)
library(randomForest)
library(caret)

```

```{r}
#change in diagnostic
path = "~/Documents/3eme-annee/Data R/Projet_Sante-/"
setwd(path)
load("dataset.Rdata")
library("neiss")
original <- as.data.frame(injuries)


#-----new dataset to make the model
## ------Onehot Encoding to factor
data.model <- data
data.model$output <- (substring(names(data[,5:30]),first = 11)[max.col(data[,5:30])])
data.model[,5:30] <- NULL
data.model$output <- as.factor(data.model$output)
#put the original vector
data.model$diag <- as.factor(original$diag)
data.model$date <- data.model$trmt_date
data.model$trmt_date <- NULL
print(str(data.model$diag))
```
1. description du dataset 
```{r}
data.model <- subset(data.model, select=c(date,age:output))
str(data.model)
```
```{r}
data.model$fmv <- NULL
data.model$psu <- NULL
data.model$weight <- NULL
data.model$narrative <- NULL
summary(data.model)
##### save
save(data.model,file="dataset_model.Rdata")
```

# Premier approach

---------------------------------
summary
## Tendance
```{r}
load("./dataset_model.Rdata")

le_month <- data.model %>%
  mutate(Year = date %>% year(),
         Month = date %>% month(),
         YearMonth = ymd(paste0(Year, "-", Month, "-01"))) %>%
  group_by(Year, YearMonth) %>%
  tally()
head(le_month)  


ggplot(le_month,aes(x=YearMonth, y=n)) +
facet_wrap(~Year, scales = "free_x", ncol = 4) +
geom_line() +
expand_limits(y = 0)+
  ggtitle("Nombre d'observations")

```
        
# count by factor  
```{r}
gather(df, key, value, -owner) %>% 
    count(owner, key, value) %>% 
    spread(value, n, fill = 0)
```
     



# ------ modele un jour

 
# factor

Model en utilisant que 10000

```{r}
index <- sample(1:nrow(data.model), 100000)
un.jour.factor <- data.model[index,]
set_size <- dim(un.jour.factor)[1]
test_size <- round(0.2 * set_size)

# to numeric
factor.jour <- Filter(is.factor,un.jour.factor)
for(col in names(factor.jour)){
  un.jour.factor[,col] <- as.numeric(un.jour.factor[,col])
}
un.jour.factor$output <- as.factor(un.jour.factor$output)

print(str(un.jour.factor))
train_data = un.jour.factor[1:(set_size - test_size),]
test_data = un.jour.factor[(set_size - test_size):set_size,]
##rpart
# Training with classification tree
# Training with Random forest model
#registerDoParallel(cl)

```





#random fr

```{r}
set.seed(12345)
rfnum <- randomForest( output ~. , data = train_data, ntree = 200)
print(rfnum)
#plot importance of variables
```
```{r}
varImpPlot(rf)
perf_test_rf <- predict(rf, test_data)

perf_test_rf
```

```{r}
rf.mil <- randomForest( output ~. , data = train_data, ntree = 1000)
print(rf.mil)
```

 ##

# NEW DATASET
## get data and cleaning
```{r}
index <- sample(1:nrow(data.model), 50000)
#un.jour <- data.model[data.model$date >= "2017-12-24",]
un.jour <- data.model[index,]
un.jour.enc <- model.matrix(output ~ ., data = un.jour)
####2. Zero- and Near Zero-Variance Predictors
#nzv <- nearZeroVar(un.jour.enc, saveMetrics= TRUE)
#top10
#print(nzv[nzv$nzv,][1:10,])

#### 3. remove zero variance
nzv <- nearZeroVar(un.jour.enc)
un.jour.filter <- un.jour.enc[, -nzv]
un.jour.enc <- NULL
dim(un.jour.filter)
```
#4. Identifying Correlated Predictors

```{r}

library(corrplot)
descrCor <-  cor(un.jour.filter)
corrplot(descrCor, type = "upper", order = "hclust", tl.col = "black", tl.srt = 45)

```
```{r}
print(sum(abs(descrCor[upper.tri(descrCor)]) > .999))
```
# data split
```{r}
set.seed(3456)
trainIndex <- createDataPartition(un.jour$output, p = .8, 
                                  list = FALSE, 
                                  times = 1)

data.train <- as.data.frame(un.jour.filter[ trainIndex,])
#data.train <- cbind(dataTrain, un.jour[ trainIndex,"output"])  
data.train$output <- un.jour[ trainIndex,"output"]

data.test <-  as.data.frame(un.jour.filter[-trainIndex,])
#data.test <- cbind(dataTest, un.jour[-trainIndex,"output"])
data.test$output <- un.jour[ -trainIndex,"output"]

#data.train$date <- NULL
#data.test$date <- NULL

##change names
names(data.train) <- gsub(" |,","",names(data.train))
names(data.test) <- gsub(" |,","",names(data.test))
names(data.test)
```

#model

```{r}
#svm
  
set.seed(12345)
rf <- randomForest( output ~. , data = data.train, ntree = 1500)
print(rf)


```
```{r}
varImpPlot(rf)
```

```{r}
plot(rf, main = "Error rate of random forest")
```
---------------
other approach

```{r}
index <- sample(1:nrow(data.model), 100000)
un.jour.factor <- data.model[index,]
set_size <- dim(un.jour.factor)[1]
test_size <- round(0.2 * set_size)

# to numeric
factor.jour <- Filter(is.factor,un.jour.factor)
for(col in names(factor.jour)){
  un.jour.factor[,col] <- as.numeric(un.jour.factor[,col])
}
un.jour.factor$output <- as.factor(data.model[index,"output"])

print(str(un.jour.factor))

##rpart
# Training with classification tree
# Training with Random forest model
#registerDoParallel(cl)
nzv <- nearZeroVar(un.jour.factor,saveMetrics = TRUE)
print(nzv[nzv$nzv,][1:10,])

nzv <- nearZeroVar(un.jour.factor)
un.jour.factor <- un.jour.factor[, -nzv]
str(un.jour.factor)

##correlation
descrCor <-  cor(un.jour.factor[,-9])
corrplot(descrCor, type = "upper", order = "hclust", tl.col = "black", tl.srt = 45)

train_data = un.jour.factor[1:(set_size - test_size),]
test_data = un.jour.factor[(set_size - test_size):set_size,]

set.seed(12345)
rf.num.fil <- randomForest( output ~. , data = train_data, ntree = 100)
print(rf.num.fil)
varImpPlot(rf.num.fil)
```
# Test taking values as factor
```{r}
load("dataset_model.Rdata")
load("dataset.Rdata")
data.full <- data.model
data.full$fmv <- data$fmv
data.full$psu <- as.factor(data$psu)
summary(data.full)
##### save
save(data.full,file="dataset_model_full.Rdata")

```
## correlation categorical

**Pearson's chi-squared test (χ2)**  is a statistical test applied to sets of categorical data to evaluate how likely it is that any observed difference between the sets arose by chance. It is the most widely used of many chi-squared tests (e.g., Yates, likelihood ratio, portmanteau test in time series, etc.)

 > The measure of association does not indicate causality, but association–that is, whether a variable is associated with another variable. This measure of association also indicates the strength of the relationship, whether, weak or strong.
 Since,nominal categorical predictor’s, the Goodman and Kruskal’s tau measure is appropriate. Interested readers are invited to see pages 68 and 69 of the Agresti book. More information on this test can be seen here
 
[link](https://www.r-bloggers.com/to-eat-or-not-to-eat-thats-the-question-measuring-the-association-between-categorical-variables/)
```{r}
load("dataset_model_full.Rdata")
library(GoodmanKruskal)
#categorical data 

varset1 <- names(Filter(is.factor,data.full))
varset1

dataFrame1<- subset(data.full, select = varset1)
#make matrix
GKmatrix1<- GKtauDataframe(dataFrame1)
plot(GKmatrix1, corrColors = "blue")
```
> The off-diagonal elements contain the forward and backward tau measures for each variable pair. Specifically, the numerical values appearing in each row represent the association measure τ(x,y)τ(x,y) from the variable xx indicated in the row name to the variable yy indicated in the column name.

> The most obvious feature from this plot is the fact that the variable odor is almost perfectly predictable (i.e. τ(x,y)=0.94) from class and this forward association is quite strong. The forward association suggest that x=odor (which has levels “almond”, “creosote”, “foul”, “anise”, “musty”, “nosmell”, “pungent”, “spicy”, “fishy”) is highly predictive of y=class (which has levels “edible”, “poisonous”). 

> x=cap shape is weakly associated to y=cap surface (i.e.τ(x,y)=0.03) and (i.e.τ(y,x)=0.01). Thus, we can safely say that although these two variables are significant but they are association is weak; i.e. it will be difficult to predict one from another.

### diag et prod1
```{r}
index <- sample(1:nrow(data.full), 200000)
data <- data.full[index,c("diag","prod1","output")]

#change produit
prod.plus <- as.data.frame(sort(table(data$prod1),decreasing = TRUE))[1:52,1]
levels(data$prod1)[!levels(data$prod1) %in% prod.plus] <- "XX"
str(data)

#split
set_size <- dim(data)[1]
test_size <- round(0.2 * set_size)
train_data = data[1:(set_size - test_size),]
test_data = data[(set_size - test_size):set_size,]

#model
set.seed(12345)
model.rf <- randomForest( diag ~. , data = train_data, ntree = 100)
print(model.rf)


```

```{r}
data.num <- data

prod.plus <- as.data.frame(sort(table(data.num$prod1),decreasing = TRUE))[1:52,1]
levels(data.num$prod1)[!levels(data.num$prod1) %in% prod.plus] <- "XX"
str(data)



#split
set_size <- dim(data.num)[1]
test_size <- round(0.2 * set_size)
train_data = data.num[1:(set_size - test_size),]
test_data = data.num[(set_size - test_size):set_size,]

#model
set.seed(12345)
model.rf.num <- randomForest( output ~. , data = train_data, ntree = 100)
print(model.rf.num)
varImpPlot(model.rf.num)

```
```{r}
data.num <- data

out.plus <- as.data.frame(sort(table(data.num$output),decreasing = TRUE))[1:15,1]
indx <- data.num$output %in% out.plus
levels(data.num$output)[!levels(data.num$output) %in% out.plus] <- "Other"
str(data.num)
data.x <- data.num

  set_size <- dim(data.x)[1]
  test_size <- round(0.2 * set_size)
  train_data = data.x[1:(set_size - test_size),]
  test_data = data.x[(set_size - test_size):set_size,]
  #model
  set.seed(12345)
  model.rf.ot <- randomForest( output ~. , data = train_data, ntree = 100)
  print(model.rf.ot)

```

