---
title: "R Notebook"
output: html_notebook
---

**Note** : 

#### Dans la base de données originale, il n'y a que 31 diagnostics reconnus, ce n'est pas une bonne idée de les laisser dans une seule colonne. Cela génère plus de 15 000 diagnostics 
```{r}
library("dplyr")
#date
library("lubridate")
#data manipulation,
library(tidyr)
library(ggplot2)
library(doParallel)
library(randomForest)

```

```{r}
#change in diagnostic
path = "~/Documents/3eme-annee/Data R/Projet_Sante-/"
setwd(path)
load("dataset.Rdata")
library("neiss")
original <- as.data.frame(injuries)


#-----new dataset to make the model
## ------Onehot Encoding to factor
data.model <- data
data.model$output <- (substring(names(data[,5:30]),first = 11)[max.col(data[,5:30])])
data.model[,5:30] <- NULL
data.model$output <- as.factor(data.model$output)
#put the original vector
data.model$diag <- as.factor(original$diag)
data.model$date <- data.model$trmt_date
data.model$trmt_date <- NULL
print(str(data.model$diag))
```
1. description du dataset 
```{r}
data.model <- subset(data.model, select=c(date,age:output))
str(data.model)
```
```{r}
data.model$fmv <- NULL
data.model$psu <- NULL
data.model$weight <- NULL
data.model$narrative <- NULL
summary(data.model)
##### save
save(data.model,file="dataset_model.Rdata")
```

# Premier approach

---------------------------------
summary
## Tendance
```{r}
load("./dataset_model.Rdata")

le_month <- data.model %>%
  mutate(Year = date %>% year(),
         Month = date %>% month(),
         YearMonth = ymd(paste0(Year, "-", Month, "-01"))) %>%
  group_by(Year, YearMonth) %>%
  tally()
head(le_month)  


ggplot(le_month,aes(x=YearMonth, y=n)) +
facet_wrap(~Year, scales = "free_x", ncol = 5) +
geom_line() +
expand_limits(y = 0)+
  ggtitle("Nombre d'observations")

```

# ------ modele un jour

 
# factor

Model en utilisant que 10000

```{r}
index <- sample(1:nrow(data.model), 100000)
un.jour.factor <- data.model[index,]
set_size <- dim(un.jour.factor)[1]
test_size <- round(0.2 * set_size)

# to numeric
factor.jour <- Filter(is.factor,un.jour.factor)
for(col in names(factor.jour)){
  un.jour.factor[,col] <- as.numeric(un.jour.factor[,col])
}
un.jour.factor$output <- as.factor(un.jour.factor$output)

print(str(un.jour.factor))
train_data = un.jour.factor[1:(set_size - test_size),]
test_data = un.jour.factor[(set_size - test_size):set_size,]
##rpart
# Training with classification tree
# Training with Random forest model
#registerDoParallel(cl)

```





#random fr

```{r}
set.seed(12345)
rfnum <- randomForest( output ~. , data = train_data, ntree = 200)
print(rfnum)
#plot importance of variables
```
```{r}
varImpPlot(rf)
perf_test_rf <- predict(rf, test_data)

perf_test_rf
```


# NEW DATASET
## get data and cleaning
```{r}
index <- sample(1:nrow(data.model), 25000)
#un.jour <- data.model[data.model$date >= "2017-12-24",]
un.jour <- data.model[index,]
un.jour.enc <- model.matrix(output ~ ., data = un.jour)
####2. Zero- and Near Zero-Variance Predictors
#nzv <- nearZeroVar(un.jour.enc, saveMetrics= TRUE)
#top10
#print(nzv[nzv$nzv,][1:10,])

#### 3. remove zero variance
nzv <- nearZeroVar(un.jour.enc)
un.jour.filter <- un.jour.enc[, -nzv]
un.jour.enc <- NULL
dim(un.jour.filter)
```
#4. Identifying Correlated Predictors

```{r}

library(corrplot)
descrCor <-  cor(un.jour.filter)
corrplot(descrCor, type = "upper", order = "hclust", tl.col = "black", tl.srt = 45)

```
```{r}
print(sum(abs(descrCor[upper.tri(descrCor)]) > .999))
```
# data split
```{r}
set.seed(3456)
trainIndex <- createDataPartition(un.jour$output, p = .8, 
                                  list = FALSE, 
                                  times = 1)

data.train <- as.data.frame(un.jour.filter[ trainIndex,])
#data.train <- cbind(dataTrain, un.jour[ trainIndex,"output"])  
data.train$output <- un.jour[ trainIndex,"output"]

data.test <-  as.data.frame(un.jour.filter[-trainIndex,])
#data.test <- cbind(dataTest, un.jour[-trainIndex,"output"])
data.test$output <- un.jour[ -trainIndex,"output"]

#data.train$date <- NULL
#data.test$date <- NULL

##change names
names(data.train) <- gsub(" |,","",names(data.train))
names(data.test) <- gsub(" |,","",names(data.test))
names(data.test)
```

#model

```{r}
#svm
  
set.seed(12345)
rf <- randomForest( output ~. , data = data.train, ntree = 1500)
print(rf)


```
```{r}
varImpPlot(rf)
```

```{r}
plot(rf, main = "Error rate of random forest")
```
---------------
other approach

# Test taking values as factor
```{r}
load("dataset_model.Rdata")
load("dataset.Rdata")
data.full <- data.model
data.full$fmv <- data$fmv
data.full$psu <- as.factor(data$psu)
summary(data.full)
##### save
save(data.full,file="dataset_model_full.Rdata")

```
## correlation categorical

**Pearson's chi-squared test (χ2)**  is a statistical test applied to sets of categorical data to evaluate how likely it is that any observed difference between the sets arose by chance. It is the most widely used of many chi-squared tests (e.g., Yates, likelihood ratio, portmanteau test in time series, etc.)

 > The measure of association does not indicate causality, but association–that is, whether a variable is associated with another variable. This measure of association also indicates the strength of the relationship, whether, weak or strong.
 Since,nominal categorical predictor’s, the Goodman and Kruskal’s tau measure is appropriate. Interested readers are invited to see pages 68 and 69 of the Agresti book. More information on this test can be seen here
 
[link](https://www.r-bloggers.com/to-eat-or-not-to-eat-thats-the-question-measuring-the-association-between-categorical-variables/)
```{r}
load("dataset_model_full.Rdata")
library(GoodmanKruskal)
#categorical data 

varset1 <- names(Filter(is.factor,data.full))
varset1

dataFrame1<- subset(data.full, select = varset1)
#make matrix
GKmatrix1<- GKtauDataframe(dataFrame1)
plot(GKmatrix1, corrColors = "blue")
```
> The off-diagonal elements contain the forward and backward tau measures for each variable pair. Specifically, the numerical values appearing in each row represent the association measure τ(x,y)τ(x,y) from the variable xx indicated in the row name to the variable yy indicated in the column name.

> The most obvious feature from this plot is the fact that the variable odor is almost perfectly predictable (i.e. τ(x,y)=0.94) from class and this forward association is quite strong. The forward association suggest that x=odor (which has levels “almond”, “creosote”, “foul”, “anise”, “musty”, “nosmell”, “pungent”, “spicy”, “fishy”) is highly predictive of y=class (which has levels “edible”, “poisonous”). 

> x=cap shape is weakly associated to y=cap surface (i.e.τ(x,y)=0.03) and (i.e.τ(y,x)=0.01). Thus, we can safely say that although these two variables are significant but they are association is weak; i.e. it will be difficult to predict one from another.


```{r}

#model
table(data.model$output)
```
1. tomar la misma cantidad por cada clase
2. nnet


```{r}
library(nnet)
## net data
levels(data.model$output)[1] <- levels(data.model$output)[2]  # remove "25% body" factor, it becomes all body
table(data.model$output)

#create a dataset with 5000 samples by factor
sel <- subset(data.model, data.model$output == levels(data.model$output)[1]) #get obs from all body
data.good <- sel[sample(1:nrow(sel),5000),]

#get from the others
for(l in levels(data.model$output)[-1]){
  sel <- subset(data.model, data.model$output == l)
  data.good <- rbind(data.good, sel[sample(1:nrow(sel),5000),])
}
### new dataset
str(data.good)
head(data.good)
save(data.good,file ="dataset_balanced.Rdata")



```

### function to plot the confusion matrix

```{r}
plot_confusion_matrix <- function(out.pred,true.label,title){
                        mat <- as.data.frame(prop.table(table(out.pred,true.label),1)) #normalize by class
                        plot <- ggplot(mat,aes(x = out.pred,y=true.label,fill=Freq))+
                                geom_tile()+
                                ggtitle(title)+
                                theme(axis.text.x = element_text(angle=45, hjust = 1))+ #incline text
                                xlab("true.label")+
                                xlab("predicted")
                        return(plot)
}
```

### new dataset diag equilibre
```{r}
levels(data.model$diag)
data.diag <- data.model[,c("date","age","diag","prod1","output")]
data.diag <- data.diag[sample(1:nrow(data.diag),nrow(data.diag)),] #permute dataset
#change all burns
lev <- gsub(" |,","",levels(data.diag$diag)) #remove spaces and ,
levels(data.diag$diag) <- gsub("^(Burns)(.+)","Burns",(lev))  #change to Burn
str(data.diag)
table(data.diag$diag)



#only the diag populares
diag.plus <- as.data.frame(sort(table(data.diag$diag),decreasing = TRUE))[1:16,1]
levels(data.diag$diag)[!levels(data.diag$diag) %in% diag.plus] <- "OtherDiag"
levels(data.diag$diag)

#only the prod populares
prod.plus <- as.data.frame(sort(table(data.diag$prod1),decreasing = TRUE))[1:50,1]
levels(data.diag$prod1)[!levels(data.diag$prod1) %in% prod.plus] <- "OtherProd"
levels(data.diag$prod1)

#create a dataset with 5000 samples by diag
sel <- subset(data.diag, data.diag$diag == levels(data.diag$diag)[1]) #get obs from diag1
data.good.diag <- sel[sample(1:nrow(sel),5000),]

#get from the others
for(l in levels(data.diag$diag)[-1]){
  sel <- subset(data.diag, data.diag$diag == l)
  data.good.diag <- rbind(data.good.diag, sel[sample(1:nrow(sel),5000),])
}
### new dataset
str(data.good.diag)
head(data.good.diag)
data.good.diag <- data.good.diag[sample(1:nrow(data.good.diag),nrow(data.good.diag)),] #permute dataset
head(data.good.diag)
save(data.good.diag, file= "dataseteq_diag.Rdata")
```

## model output

```{r}
load("dataset_balanced.Rdata")


#split
dataset <- data.good[,c("date","age","diag","prod1","output")]
dataset <- dataset[sample(1:nrow(dataset),nrow(dataset)),] #permute dataset

#only the 52 products most populars
prod.plus <- as.data.frame(sort(table(dataset$prod1),decreasing = TRUE))[1:52,1]
levels(dataset$prod1)[!levels(dataset$prod1) %in% prod.plus] <- "XX"
print(str(dataset))
#model
plot(table(dataset$output))

test_size <- round(0.7 * nrow(dataset))
train_data = dataset[1:test_size,]
test_data = dataset[-(1:test_size),]

set.seed(12345)
model.grf <- randomForest( output ~. , data = train_data, ntree = 100)
print(model.grf)
varImpPlot(model.grf)


```

### autotune?
```{r}
set.seed(12345)
bestmtry <- tuneRF(train_data[,-5],train_data$output, stepFactor=1.5, improve=1e-5, ntree=100)
print(bestmtry)

#model 50 trees mtry2
set.seed(12345)
model.grf.2 <- randomForest( output ~. , data = train_data, ntree = 50, mtry =2)
print(model.grf.2)
varImpPlot(model.grf.2)

#model 50 trees mtry2 without date
set.seed(12345)
model.grf.nd <- randomForest( output ~ diag +age+prod1 , data = train_data, ntree = 50,mtry =2)
print(model.grf.nd)
varImpPlot(model.grf.nd)

```


```{r}
set.seed(12345)
model.grf4 <- randomForest( output ~. , data = train_data, ntree = 500)
print(model.grf4)
```

##results

```{r}
out.pred2 <- predict(model.grf.2, train_data[,-5]) #all data, mtry 2, 50nt
out.pred3 <- predict(model.grf.nd, train_data[,-5]) #without data, mtry2, 50nt
out.pred1 <- predict(model.grf, train_data[,-5]) #all data, mtry auto, 100nt
out.pred4 <- predict(model.grf4, train_data[,-5]) #all data, mtry auto, 100nt

p1 <- plot_confusion_matrix(out.pred1,train_data$output,"CM of output (training_data) m1")
p2 <- plot_confusion_matrix(out.pred2,train_data$output,"CM of output (training_data) m2")
p3 <- plot_confusion_matrix(out.pred3,train_data$output,"CM of output (training_data) m3")
p4 <- plot_confusion_matrix(out.pred4,train_data$output,"CM of output (training_data) m4")

##test
out.pred2 <- predict(model.grf.2, test_data[,-5]) #test data without output
out.pred3 <- predict(model.grf.nd, test_data[,-5]) #test data without output
out.pred1 <- predict(model.grf, test_data[,-5]) #test data without output
out.pred4 <- predict(model.grf4, test_data[,-5]) #test data without output

p5 <- plot_confusion_matrix(out.pred1,test_data$output,"CM of output (test_data) m1")
p6 <- plot_confusion_matrix(out.pred2,test_data$output,"CM of output (test_data) m2")
p7 <- plot_confusion_matrix(out.pred3,test_data$output,"CM of output (test_data) m3")
p8 <- plot_confusion_matrix(out.pred4,test_data$output,"CM of output (test_data) m4")

library(cowplot)
figure1 <- plot_grid(p1,p5,p2,p6,p3,p7,p4,p8,
                    ncol = 2, nrow = 4,
                    align = "hv")
figure1

save_plot("cm_rf_output.png", figure1, ncol = 2, nrow = 4)
```

### model diagnostic

```{r}
####
load("dataseteq_diag.Rdata")
test_size <- round(0.7 * nrow(data.good.diag))
train_data = data.good.diag[1:test_size,]
test_data = data.good.diag[-(1:test_size),]

set.seed(12345)
model.diag.rf <- randomForest( diag ~. , data = train_data, ntree = 100)
print(model.diag.rf)
set.seed(12345)
model.diag.rf2 <- randomForest( diag ~ output + prod1 , data = train_data, ntree = 100)
print(model.diag.rf2)

set.seed(12345)
model.diag.rf3 <- randomForest( diag ~. , data = train_data, ntree = 50, mtry =2)
print(model.diag.rf3)

varImpPlot(model.diag.rf)
varImpPlot(model.diag.rf2)
varImpPlot(model.diag.rf3)


```
### confusion matrix

```{r}
str(train_data)
out.pred1 <- predict(model.diag.rf, train_data[,-3]) #all data, 100nt
out.pred2 <- predict(model.diag.rf2, train_data[,-3]) #only prod1 et output 100nt
out.pred3 <- predict(model.diag.rf3, train_data[,-3]) #all data, mtry 2, 50nt

p1 <- plot_confusion_matrix(out.pred1,train_data$diag,"CM of diagnostic (training_data) m1")
p2 <- plot_confusion_matrix(out.pred2,train_data$diag,"CM of diagnostic (training_data) m2")
p3 <- plot_confusion_matrix(out.pred3,train_data$diag,"CM of diagnostic (training_data) m3")


##test
out.pred1 <- predict(model.diag.rf, test_data[,-3]) #test data without diag
out.pred2 <- predict(model.diag.rf2, test_data[,-3]) #test data without diag
out.pred3 <- predict(model.diag.rf3, test_data[,-3]) #test data without diag

p4 <- plot_confusion_matrix(out.pred1,test_data$diag,"CM of diagnostic (test_data) m1")
p5 <- plot_confusion_matrix(out.pred2,test_data$diag,"CM of diagnostic (test_data) m2")
p6 <- plot_confusion_matrix(out.pred3,test_data$diag,"CM of diagnosticut (test_data) m3")

library(cowplot)
figure1 <- plot_grid(p1,p4,p2,p5,p3,p6,
                    ncol = 2, nrow = 3,
                    align = "hv")
figure1

save_plot("cm_rf_diag.png", figure1, ncol = 2, nrow = 3)
```



